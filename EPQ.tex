\documentclass[10pt,a4paper]{report}

%Packages
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsfonts,amssymb,amsthm}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{chngcntr}
\usepackage[titletoc]{appendix}
\usepackage{listings}
\usepackage{xcolor}

%Citations
\usepackage{biblatex}
\addbibresource{references.bib}

%Argmin and argmax
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

%Images and Graph Plots
\usepackage{graphicx}
\graphicspath{{figures/}}
\usepackage{tikz, pgf, pgfplots}
\pgfplotsset{width=10cm,compat=newest}
\usepgfplotslibrary{external}
\tikzexternalize[prefix=tikz/]

%Code Segments
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{code}{
	backgroundcolor=\color{backcolour},
	commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\ttfamily\footnotesize,
	breakatwhitespace=false,
	breaklines=true,
	captionpos=b,
	keepspaces=true,
	numbers=left,
	numbersep=5pt,
	showspaces=false,
	showstringspaces=false,
	showtabs=false,
	tabsize=2
}
\lstset{style=code}

%Hyperlinks
\usepackage[hidelinks, pdfpagelabels]{hyperref}
\hypersetup{pageanchor=true}

\setlength{\parindent}{0mm}
\setlength{\parskip}{2mm}

%Document Information
\title{What are the limitations of derivative-based \\
	   models for optimization in machine learning?}
\author{Faris Chaudhry}
\date{\today}

%Environments
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem*{definition}{Definition}
\counterwithout{equation}{chapter}

%Page Numbering
\newcommand\frontmatter{
	\cleardoublepage
	\pagenumbering{roman}}
\newcommand\mainmatter{
	\cleardoublepage
	\pagenumbering{arabic}}

\begin{document}
	\frontmatter
	\maketitle

	\begin{abstract}
		Most machine learning problems can be transposed into optimization problems
		with the goal being finding the global minima or maxima to minimize loss or maximize potential.
		Each of the main learning methodologies (namely supervised, unsupervised and reinforcement)
		along with the models to represent and solve optimization problems have limitations -
		computationally and theoretically - that have to be identified and mitigated against
		to create an effective model. The focus here are objective functions that are continuously differentiable
		and thus derivative-based solutions are used.
	\end{abstract}

	\tableofcontents
	\mainmatter

    \chapter{Introduction to Machine Learning and Optimization}

		\section{What is Machine Learning?}
			Machine learning (ML) is a subfield of artificial intelligence (AI) which,
			broadly speaking, is the use of computational methods and models to improve
			performance and predictions through experience \autocite[p. 1]{FoundationsOfMachineLearning}.
			Unlike humans, this learning is based entirely on data and statistics and
			experience is gained through interaction with a training set of data
			or an environment of some kind. \par
			\begin{figure}[h]
				\centering
				\includegraphics[scale=0.8]{ai-fields-euler-diagram.png}
				\caption{subfields of AI}
				\label{fig:ai-subfields}
			\end{figure}
			There are 3 primary categories (supervised, unsupervised and reinforcement) of learning philosophies for ML models,
			with other hybrid models being combinations of these. Each type of learning lends to itself to certain types of problems due to
			the limitations that each one has. Supervised learning is used for classifying images and extrapolating data. Unsupervised learning
			takes raw data and finds patterns such as the overall distribution or groups with similar attributes. Reinforcement is used in complex
			systems which many changing variables that would be computationally difficult to solve otherwise, like chess.

		\section{Prerequisite Conditions for Derivative-Based Optimization}
			Optimization revolves around minimizing the loss or maximizing the value of a function. In the
			context of ML, the process of optimizing is vital to ensure modelling produces the greatest accuracy.
			The goal is to optimize an objective function, which is the
			representation of the variables being simulated. The solution to the objective function will be
			either a minimum (minima) or maximum (maxima) point (collectively called the set of extrema) as
			this is when the value of a function is highest or lowest. \par
			The derivative is a linear approximation (tangent) to a function at a point. Suppose there was a
			function $f(x)$ then, intuitively, the derivative with respect to $x$ would be the how much the value of $f(x)$
			changed with a small nudge in the $x$ direction. It is important to note that, by Fermat's theorem on stationary points
			\autocite{StationaryPoints}, all critical points (extrema and saddle points) have first derivative equal to 0. Visually, this
			is because the tangent to any turning point will have a gradient of 0. See the function $y = x^2$ (fig. \ref{fig:extrema}) which has a minima at $(0,0)$. \par
			\begin{figure}[h]
				\centering
				\begin{tikzpicture}
					\begin{axis}[
						width=5cm,
						height=5cm,
						xtick=\empty,
						ytick=\empty,
						xlabel=$x$,
						ylabel=$y$,
						axis lines=center,
						domain=-2.5:2.5,
						samples=100]
						\addplot [red] {x^2};
					\end{axis}
				\end{tikzpicture}
				\caption{graph of $x^2$}
				\label{fig:extrema}
			\end{figure}
			So the objective becomes to find all critical point locations and the nature of these points - what kind of
			critical point it is.

			\subsection{Continuity and Differentiability}
				The most essential requirement to using derivative-based methods will be that the objective
				function must be continuous and twice-differentiable (the derivative of the function must also be differentiable)
				over the interval that contains the solution. \par
				This is because to find the location and nature of critical points, the first and second derivative
				of a function are required \autocite{SecondDerivativeTest}. \par


				For a function $f(x)$ to be continuous over the interval $I = [a,b]$
				\begin{equation}
					\forall k \in I, \lim_{x \to k} f(x) = f(k)
					\label{eq:continuity}
				\end{equation}
				This means that, given any number in the interval, as $x$ approaches that number it would be equal to putting
				the number into the function. This prevents any discontinuity since the limit wouldn't exist at discontinuous points. In fig. \ref{fig:discontinuity},
				$\lim_{x \to 0^+} = +\infty$ and $\lim_{x \to 0^-} = -\infty$. These values contradict meaning the limit isn't defined.
				\begin{figure}[h]
					\centering
					\begin{tikzpicture}
						\begin{axis}[
							width=5cm,
							height=5cm,
							xtick=\empty,
							ytick=\empty,
							xlabel=$x$,
							ylabel=$y$,
							axis lines=center,
							domain=-2.5:2.5,
							samples=100]
							\addplot [red] {1/x};
						\end{axis}
					\end{tikzpicture}
					\caption{graph of $\frac{1}{x}$}
					\label{fig:discontinuity}
				\end{figure}

				For a single-valued function, $f(x)$, the derivative, $f'(x)$ exists iff the following limits exists.
				\begin{equation}
					\frac{df}{dx} = \lim_{\Delta x \to 0} \frac{f(x+\Delta x) - f(x)}{\Delta x}
					\label{eq:first-principles-uni}
				\end{equation}
				However, most objective functions will be multi-valued to account for all the variables so this
				definition must be extended. This is the same principle but a nudge in a specific direction. Suppose
				there is a function $f(x_1,\cdots,x_i)$ then the derivative with respect to a certain variable, $x_n$, will be.
				\begin{equation}
					\frac{\partial f}{\partial x_n} = \lim_{x \to 0} \frac{f(x_1,\cdots,x_n+\Delta x,\cdots,x_i) - f(x_1, \cdots,x_n,\cdots,x_i)}{\Delta x}
					\label{eq:first-principles-multi}
				\end{equation}
				In practice these rigorous definitions are not used but the concept of continuous differentiability is important.
				\begin{itemize}
					\item The first and second derivative must exist for an objective function to be solvable in this method,
					which is the major limiting factor. Although derivative-free methods do exist,
					they tend to be approximations of the exact values and heuristic in theory.
					\item Although many functions discontinuities, like asymptotes or singularities, many times these are
					removable either by defining an interval without them or assigning an arbitrary value at a point for continuity.
				\end{itemize}

			\subsection{Concavity and Convexity}
				When a function has only 1 minima or maxima over an interval it becomes much easier to find the global minimum
				or maximum due the lack of a need to check which point is a local extremum and which is the global extremum. Functions like these are
				called convex and concave where convex functions have a minimum point and concave functions have a maximum point.
				A convex function \autocite{vandenberghe2004convex} can visually be described as having all its points below a line segment drawn
				between (fig. \ref{fig:convexity}) any 2 points while a concave function has all points above. \par
				It is important to note that concavity and convexity are not opposites. A function can be concave, non-concave, convex or non-convex.
				In addition, reflecting a function in the $x$-axis will reverse its concavity or convexity. Suppose $f(x)$ is convex then $-f(x)$ is concave.
				\begin{figure}[h]
					\centering
					\begin{tikzpicture}
						\begin{axis}[
							width=5cm,
							height=5cm,
							xtick=\empty,
							ytick=\empty,
							xlabel=$x$,
							ylabel=$y$,
							axis lines=center,
							domain=-2.5:2.5,
							samples=100]
							\addplot [red] {x^4 + x^3 - 2 * x^2 - 2*x};
							\addplot [blue] {x^2};
						\end{axis}
					\end{tikzpicture}
					\caption{convex (blue) and non-convex (red)}
					\label{fig:convexity}
				\end{figure} \\
				If the objective function is a non-convex or non-concave function this doesn't prevent the use of derivative-based optimization. However
				it does restrict the range of methods that can be used to find the global solution. For example, iterative methods to find extrema might not always work
				since they could get closer to the local extrema while neglecting other possible values. Moreover, it will increase the complexity of the problem
				computationally since there will be range of possible global extrema that have to be checked - which can be particularly difficult when
				certain derivative tests are inconclusive.

	\chapter{Supervised Learning}

		\section{Application of Supervised Learning}
			The philosophy of supervised learning is to use labelled training data to map between
			an input vector and a target vector. In this case, the model is given
			data with input variables and the correct associated target values corresponding with them \autocite[p. 105]{DeepLearning}.
			Effectively, the model is creating a pattern out of which inputs cause certain outputs so that,
			given new inputs, the correct outputs can be predicted. \par
			Supervised learning problems are split into 2 distinct categories: classification problems and regression problems.

			\subsection{Classification}
				Classification problems are about predicting the class labels of an object. A common example
				of classification is assigning an digit label to a handwritten digit. However, these objects
				could be anything that can be labelled such as sentences or sounds. \par
				Let $x_n$ be a feature/parameter of the object and $l_n$ be a label where $n \in \mathbb{Z}$\\
				Then a general classification function can be described as the mapping: \[[x_1,x_2,\cdots] \mapsto [l_1,l_2,\cdots]\]
				Given a particular feature vector (a vector of the parameters of the classification function) the goal is to assign a set of class labels.
				\begin{figure}[h]
					\centering
					\includegraphics[scale=0.5]{classification-diagram.png}
					\caption{example of classification}
					\label{fig:classifcation}
				\end{figure}
			\subsection{Regression}
				Regression problems involve predicting a numerical value from the feature vector of an object.
				For example, given many variables about a stock (past history), predict the future value of the stock.\par
				Let $x_n$ be a feature/parameter of the object and $k$ be the numerical value associated with it where $n \in \mathbb{Z}$\\
				Then a general regression function can be described as the mapping: \[[x_1,x_2,\cdots] \mapsto k\]
				\begin{figure}[h]
					\centering
					\includegraphics[scale=0.5]{regression-diagram.png}
					\caption{example of regression}
					\label{fig:regression}
				\end{figure}

		\section{General Optimization of Supervised Learning Problems}
			The optimization of a supervised learning problem is to minimize the average of the loss function using the training samples. This produces the
			most accurate approximation to the underlying function to extrapolate values. \par
			The general equation \autocite[p. 3]{SurveyOfOptimizationMethods} for this can be written as:
			\begin{equation}
				\min_\theta \frac{1}{N} \sum_{i=1}^{N} L(y^i, f(x^i,\theta))
				\label{eq:supervised-learning-general}
			\end{equation}
			where $N$ is the number of training samples, $\theta$ is the parameter of the mapping function, $x^i$ is a feature vector
			and $y^i$ is the array of labels associated with that feature vector. \par
			The problem with using training samples is that the resulting function might be over fitted to the given data. This would mean
			that, although the model is accurate for the training data it has been given, accuracy is reduced on new objects. The method to
			deal with this is through a regularization item, $\lambda$:
			\begin{equation}
				\min_\theta \frac{1}{N} \sum_{i=1}^{N} L(y^i, f(x^i,\theta)) + \lambda\| \theta\|_{2}^{2}
				\label{eq:supervised-learning-regularization}
			\end{equation}
			Regularization fundamentally discourages learning complex models and will be covered more in depth when talking about limitations.

		\section{Limitations of Supervised Learning}
			Models don't generalize well from observed, training data to unseen data \autocite{OverfittingSupervisedLearning}. The accuracy might be near perfect
			on training data while being poor on unseen data somewhat mimicking the model memorizing the training data without grasping the mapping function.






	\chapter{Unsupervised Learning}

			\section{Application of Unsupervised Learning}
			Unsupervised learning is different to supervised learning in the way that it uses unlabelled data \autocite[p. 105]{DeepLearning}; instead
			of learning from a mapping of inputs to a know output, the model is given only the inputs to learn from and has to make sense of the data without guidance.
			As a result of this, unsupervised learning revolves around extracting relationships from the data without the inherent human biases caused by choosing the correct output beforehand. \par
			Unsupervised learning problems strive to solve 1 of 2 problems: finding clusters of similar data and summarizing the distribution of the data
			(density estimation).

				\subsection{Clustering}
					Unlike classification, where the classes are predefined, clustering requires the model to define its own cluster of data
					based on the similarites of the features.
					\begin{figure}[h]
						\centering
						\includegraphics[scale=0.7]{clustering-diagram.png}
						\caption{example of clustering}
						\label{fig:clustering}
					\end{figure} \\
					Optimization will involve making the variance of each cluster as small as possible; variance will
					be equivilant to the distnace from the center of the cluster. This can be done by iterating through each cluster
					and checking the distance from the centre of the cluster for each sample.
					\begin{equation}
						\min_s \sum_{k=1}^{K} \sum_{x\in S_k} \| x - \mu_k \|_{2}^{2}
						\label{eq:unsupervised-learning-clustering}
					\end{equation}
					where $s$ is the variance, $K$ is the number of clusters, $S_k$ is the set of samples for that cluster,
					$\mu_k$ is the centre of a cluster \autocite[p. 3-4]{SurveyOfOptimizationMethods}.

				\subsection{Density Estimation}
					The assumption is that there exists some probability istribution to describe the relationship between the variables \autocite{sheather2004density}.
					Density estimation is a useful asset in modelling to estimate the properties of a given data set (variance, skewness, type of distribution). \par
					Suppose there exists a set of continous random variables, $(x_1,\cdots,x_n)$, then there is a
					probability distribution that the set models, $P(x_1,\cdots,x_n)$. The goal is to find a continous probability density function (PDF)
					that can describe the mapping: $\{x_1,\cdots,x_n\} \to P(x_1,\cdots,x_n)$. The assumption that this will PDF will be continous is valid
					since our objective function has the precondition of being continous.
					\begin{figure}[h]
						\centering
						\includegraphics[scale=0.2]{density-estimation-diagram.jpg}
						\caption{example of density estimation}
						\label{fig:density-estimation}
					\end{figure} \\
					Getting an exact function approximation is not only unlikley but also suboptimal. As more data is added the approximation will
					get more complex and usually closer to the actual value however too much data increases the risk of overfitting, especially if there is
					abnormal data. Instead, the focus is to maximize the likelihood that a predicted PDF is correct. \par
					In statistics, the likelihood function measure the goodness of fit between the data and the proposed PDF so maximizing it finds the PDF which
					has th higest probability of fitting the data.
					Principally, a likelihood function is written as $p(x^i,\theta)$ where $x^i$ is a datapoint and $\theta$ is a PDF.
					However, many data sets have large  numeric ranges that can limit the effectivness of this function so it is easier to work with the
					logarithmcic likelihood function, $\ln p(x^i,\theta)$ or $\ell(x^i,\theta)$. \par
					It is important to note that
					\begin{equation}
						\max p(x^i,\theta) = \max \ell(x^i,\theta) = \widehat{\ell}(x^i, \widehat{\theta})
					\end{equation}
					Since $\ln$ is strictly increasing then maximizing the logarithmcic likelhood function also maximizes the likelhood function. Thus general density estimation \autocite[p. 4]{SurveyOfOptimizationMethods} is:
					\begin{equation}
						\max \sum_{i=1}^N \ell(x^i,\theta)
					\end{equation}
					where $N$ is the number of training samples and $x^i$ is a particular feature vector of a sample.

			\section{Limations of Unsupervised Learning}


	\chapter{Reinforcement Learning}
		Reinforcement learning entails an agent interacting with an environment rather than using a traditional dataset \autocite[p. 105]{DeepLearning}.
		The learning proccess is done through trial and error until there is a feedback loop between the environment and the agent's experience. Each situation
		or state that the environment can have should be mapped to an action that the agent can take to maximize reward. \par
		This game theory approach assigns a payoff yield to each action that encourages or discourages certain actions.
		A byproduct of this is to consider how a length of time can affect the payoff of an action. For example, maybe an action has
		a high reward when considering only the next state but when looking at the next 10 states its value is lower. This can be seen
		in chess when, as the depth of the AI get higher, certain moves become worse since they compromise future positions. \par
		The dynamic nature of reinforcement learning is useful in complex systems where computationally working out the whole game would be inefficent if not impossible.
		Consider chess \autocite{shannon1950xxii} where the number of possible games is estiamted as $10^{123}$ (Shannon's number) and is too big to compute after even 5 turns.
		\begin{figure}[h]
			\centering
			\begin{tabular}{||c c||}
				\hline
				Number of half-moves & Number of Possible Games\\ [0.5ex]
				\hline\hline
				1 & 20\\
				\hline
				2 & 400\\
				\hline
				3 & 8,902\\
				\hline
				4 & 197,281\\
				\hline
				5 & 4,865,609\\[1ex]
				\hline
			\end{tabular}
			\caption{the exponential growth of possible chess games}
			\label{fig:possible-chess-games}
		\end{figure}


		\section{Optimization in Reinforcement Learning}
			A policy function, $\pi(s)$ maps a state, $s$, to an action, $a$, to select the best course of action from the set of all actions, $A$, that the agent
			can take in each situation from the set of possible situations, $S$.
			\[\pi(s) : s \to a \ \text{where} \ s\in S, a \in A\]
			By maximizing the excpected value of this function \autocite[p. 4]{SurveyOfOptimizationMethods}, the agent will select the
			best action to be performed in each state, maximizing the payoff of the actions.
			\begin{equation}
				\max_{\pi(s)} = \mathbf{E} \left[\sum_{k=1}^{T} \gamma^k r_{t+k} | S_t = s \right]
				\label{eq:reinforcement-learning}
			\end{equation}
			where $T$ is the time horizon, $\gamma$ is the discount factor, $r$ is the reward function with respect to the turn and
			the time into the future being considered, $S_t$ is a given state. \par
			If the game was infinite - or at least has no predetermined stopping point -
			then a simple solutions is to work out the $\lim_{T \to \infty}$ and truncate the series at some point to get a good approximation.
			Using this method of truncation would allow the depth to be changed (how far into the future turns are considered). \par
			The discount factor, $\gamma$, is an value that prioritizes instant reward over a future reward \autocite{sozou1998hyperbolic}.
			If there is a constant risk which may cause failure to realize the reward then that future reward should
			have a decreased payoff to compenstate for the risk. For example, suppose there is a 50\% chance (implying $\gamma = 0.5$) that the game ends after every
			turn and you could choose either a payoff of 1 unit after 1 turn or a payoff of 10 units after 5 turns. Then the expected value of option 1 is
			$0.5^1 * 1$ which is 0.5 and option 2 is $0.5^5 * 10$ which is 0.3125 so option 1 is statistically better.

			\subsection{Limitations of Reinforcement Learning}



	\chapter{Mathematical Models for Optimization}

		\section{Constraints on the Objective Function}
			Constraints are conditions that the soultions to the objective function must statisfy. In many cases, there are
			certain restrictions that should be added due to computational and resource limitations. \par
			There are 3 types of constraints that must be considered:
			\begin{itemize}
				\item inequality constraints such as $x \geq k$.
				\item equality constraints such as $x = k$
				\item data type constraints such as $x$ is an integer ($x \in \mathbb{Z}$)
			\end{itemize}
			Data type constraints are normally easy to deal with; changing with values the model checks or adding a conditional statement
			can be enough in many cases. \par
			Equality constraints can be appended onto the objective function using the Lagrange multiplier \autocite{LagrangeMultiplier}. Let
			$f(x)$ be an objective function and $c_{n}(x) = k_n$ be an equality constraint. Then the general lagrange multiplier for $n$ constraints would be:
			\begin{equation}
				\mathcal{L} (x) = f(x) - \lambda_1(c_{1}(x) - k_1) - \cdots - \lambda_n(c_{n}(x) - k_n)
			\end{equation}
			The solution to the original objective function, $f(x)$ will be a saddle point on the lagrange multiplier. One problem with this
			approach is that, firstly, for each constraint added there will be an extra partial derivative that needs to be computed and thus a more
			complex simultaneous equation. In addition, inequality constarints ($c(x) \geq k$) are not supported through this method. However, there
			is a generalization to the lagrange multiplier called the Karush–Kuhn–Tucker conditions (KKT).

		\section{Methods for Finding Extrema}
			\subsection{Jacobian}
				Previously, we asscertained that the locations of the critical points required the first derivative of the function to be 0 ($f' = 0$).
				For a single-valued function this is equivilent to solving \[\frac{dy}{dx} = 0\]
				However for multi-valued functions a critical point will require all partial derivatives to be 0 at that point. The Jacobian \autocite{Jacobian} is
				a vector of the parital derivatives of the function that gives a vector of the gradients at that point.
				\begin{equation}
					J_f = \nabla f = [\partial_{x_1} f, \cdots, \partial_{x_n} f]
					\label{eq:jacobian}
				\end{equation}
				This can be solved by either setting each partial derivative to 0 and solving it simultaneously or by finding the values
				when the value of the jacobian is 0. \[\partial_{x_1} f = \cdots = \partial_{x_n} f = 0 \ \textrm{or} \ \| J \| = 0 \]
				Sometimes the resulting equations might be difficult to solve (polynomials of order greater than 4 for example) and brute force or numerical methods
				might need to be used.
			\subsection{Hessian}
				After finding the locations of the critical points, higher order derivative tests must be used to determine the nature of the points.
				The Hessian \autocite{Hessian} is like the 'jacobian of the jacobian' and is a representation of the change in the gradient.
				\begin{equation}
					H_f = \nabla(\nabla f) =
						\begin{bmatrix}
							\frac{\partial^2 f}{\partial x_{1}^{2}} & \frac{\partial^2 f}{\partial x_1 \partial x_2} &\cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n}\\
							\frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_{2}^{2}} & \cdots & \frac{\partial^2 f}{\partial x_2 \partial x_n}\\
							\vdots & \vdots  & \ddots & \vdots \\
							\frac{\partial^2 f}{\partial x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_{n}^{2}}
						\end{bmatrix}
					\label{eq:hessian}
				\end{equation}
				Due to the commutative nature of second partial derivatives, computation time is significantly cut down as only the upper or lower
				triangular matrix has to be calculated and then mirrored.
				\begin{equation}
					\frac{\partial^2 f}{\partial_y \partial_x} = \frac{\partial^2 f}{\partial_x \partial_y}
					\label{eq:symmetry}
				\end{equation}
			\subsection{Higher-Order Derivative Tests}
				Using the determinant and trace of the Hessian matrix where $\lambda$ is an eigenvalue
				\begin{equation}
					trH = \sum_{i=1}^n H_{ii} = \prod_{\forall i} \lambda_i \ \text{and} \ detH = \sum_{\forall i} \lambda_i
				\end{equation}
				the second derivaitve test is as follows.
				Let $(x_1,x_2,\cdots,x_n)$ be a critical point substituted into the Hessian. Then \autocite{SecondDerivativeTest}
				\begin{itemize}
					\item $detH > 0$ and $trH > 0 \implies$ local minimum.
					\item $detH > 0$ and $trH < 0 \implies$ local maximum.
					\item  $detH < 0 \implies$ saddle point.
					\item $detH = 0 \implies$ inconclusive test.
				\end{itemize}
				Doing this for all critical points will determine which are extrema and comparing between the extrema will find the global
				minimum or maximum, which will be the solution to the optimization problem. In the case when the test is inconclusive, higher-order
				tests \autocite{ExtremumTest} can be used (although depending on the variables many derivatives would have to be calculated) or inspection. \par
				For complex functions, visual inspection is usually impossible (since displaying something with greater than 3 dimensions is unintuivtive) but numerical inspection can be useful, like
				adding $dx_n$ for each variable and comparing the values. For example, if the neighbourhood of a point has values greater then the point then it must a be minima. \par
				The Hessian matrix is not without limitations; the computing time required to work out the Hessian scales greatly as each new variable is added and, for each critical
				point, the eigenvalues of the matrix will have to be worked out to find the trace and determinant. To mitgate this there are algorithims which can approximate the Hessian
				with the only condiiton that the matrix is invertible (the determinant is not 0).

			\subsection{Iterative Methods}
					When exact solutions are too difficult to calculate either due to too many variables or the Hessian matrix being to large
					to reasonably store, iterative methods can be used as numerical approximations. Since the approximation will converge to the
					answer as the number of iterations approaches infinity, many iterative methods are far superior, in practice, because of the lower
					resource cost relative to the accuracy of the approximation. \par
					%TODO newtons methdod for optimization

\listoffigures
\printbibliography[title=References]
\end{document}
